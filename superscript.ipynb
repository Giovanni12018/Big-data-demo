{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python superscript\n",
    "\n",
    "#### Table of content\n",
    "\n",
    "    1 Spark\n",
    "    1.1 Load modules/packages\n",
    "    1.2 Spark connection setup\n",
    "    1.3 RDD operations\n",
    "    1.4 DF's and data-manipulation\n",
    "    1.5 Spark SQL\n",
    "    1.6 Machine learning pipelines\n",
    "    1.7 Correlation\n",
    "    1.8 Logistic regression\n",
    "    1.9 Recommendation system\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load modules/packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the neccesary modules/packages\n",
    "from pyspark import SparkContext\n",
    "from __future__ import print_function\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### 1.2 Spark connection setup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkContext from pyspark\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sc\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 RDD operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a list with names and values \n",
    "data = sc.parallelize([('Jordan',100),('Jason',150),('Jack',200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data example \n",
    "data_example = sc.parallelize([('Jordan',100),('Jason',150),('Jack',200)]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jordan', 100), ('Jason', 150), ('Jack', 200)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the data structure of the RDD\n",
    "data_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Jack', 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returning 'Jack'\n",
    "data_example[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a new list\n",
    "data_second = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a distributed dataset\n",
    "distData = sc.parallelize(data_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Reducing the elements in the list by adding them up\n",
    "distData.reduce(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 DF's and data-manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting and initiating the Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"Python Spark SQL basic example\") \\\n",
    " .config(\"spark.some.config.option\", \"some-value\") \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in and attach the CSV\n",
    "df = spark.read.csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the datatype\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run summary statistics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the naâ€™s in the dataframe\n",
    "df.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling null values\n",
    "df.fillna(0).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Michael| 40|\n",
      "|   Andy| 30|\n",
      "| Justin| 23|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"people.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the schema in tree format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting only the \"name\" column\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     41.0|\n",
      "|   Andy|     31.0|\n",
      "| Justin|     24.0|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting everybody, incrementing Age by 1\n",
    "df.select(df['name'], df['age'] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Michael| 40|\n",
      "|   Andy| 30|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting people older than 23\n",
    "df.filter(df['age'] > 23).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 30|    1|\n",
      "| 23|    1|\n",
      "| 40|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting people by age\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame as temporary view\n",
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Michael| 40|\n",
      "|   Andy| 30|\n",
      "| Justin| 23|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the DataFrame as global temporary view\n",
    "df.createGlobalTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Michael| 40|\n",
      "|   Andy| 30|\n",
      "| Justin| 23|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global temporary view tied to database global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Michael| 40|\n",
      "|   Andy| 30|\n",
      "| Justin| 23|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global temporary view cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The schema is encoded in a string.\n",
    "schemaString = \"name age\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "#Run SQL over DataFrames that are registered as tables.\n",
    "results = spark.sql(\"SELECT name FROM people\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and saving parquet\n",
    "df = spark.read.load(\"C:/big-datademo/superscripts/data/users.parquet\")\n",
    "df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and saving json\n",
    "df = spark.read.load(\"C:/big-datademo/superscripts/data/people.json\", format=\"json\")\n",
    "df.select(\"name\", \"age\").write.save(\"namesAndAges.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and saving CSV\n",
    "df = spark.read.load(\"C:/big-datademo/superscripts/data/people.csv\",\n",
    "                     format=\"csv\", sep=\":\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running SQL on files directly\n",
    "df = spark.sql(\"SELECT * FROM parquet.`C:/big-datademo/superscripts/data/users.parquet`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketing\n",
    "df.write.bucketBy(42, \"name\").sortBy(\"favorite_numbers\").saveAsTable(\"people_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning\n",
    "df.write.partitionBy(\"favorite_color\").format(\"parquet\").save(\"namesPartByColor.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning and bucketing on a single table\n",
    "df = spark.read.parquet(\"C:/big-datademo/superscripts/data/users.parquet\")\n",
    "(df\n",
    "    .write\n",
    "    .partitionBy(\"favorite_color\")\n",
    "    .bucketBy(42, \"name\")\n",
    "    .saveAsTable(\"people_partitioned_bucketed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Andy|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF = spark.read.json(\"C:/big-datademo/superscripts/data/people.json\")\n",
    "\n",
    "# Save dataframes as Parquet files, maintaining the schema information.\n",
    "peopleDF.write.parquet(\"people.parquet\")\n",
    "\n",
    "# Reading in the Parquet file created above, preserving the schema. Result=Dataframe\n",
    "parquetFile = spark.read.parquet(\"people.parquet\")\n",
    "\n",
    "# Using parquet files to create a temporary view and then apply SQL statements.\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "Adults = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 23 AND age <= 60\")\n",
    "Adults.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- double: long (nullable = true)\n",
      " |-- single: long (nullable = true)\n",
      " |-- triple: long (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Creating a simple DataFrame, stored into a partition directory\n",
    "sc = spark.sparkContext\n",
    "\n",
    "squaresDF = spark.createDataFrame(sc.parallelize(range(1, 6))\n",
    "                                  .map(lambda i: Row(single=i, double=i ** 2)))\n",
    "squaresDF.write.parquet(\"data/test_table/key=1\")\n",
    "\n",
    "# Creating another DataFrame in a new partition directory,\n",
    "# adding a new column and dropping an existing column\n",
    "cubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\n",
    "                                .map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "cubesDF.write.parquet(\"data/test_table/key=2\")\n",
    "\n",
    "# Reading the partitioned table\n",
    "mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\n",
    "mergedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Andy|\n",
      "+----+\n",
      "\n",
      "+----------------+----+\n",
      "|         address|name|\n",
      "+----------------+----+\n",
      "|[Columbus, Ohio]| Yin|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Pointing to a json dataset by path.\n",
    "path = \"C:/big-datademo/superscripts/data/people.json\"\n",
    "peopleDF = spark.read.json(path)\n",
    "\n",
    "# Visualizing the inferred schema\n",
    "peopleDF.printSchema()\n",
    "\n",
    "# Creating a temporary view using the DataFrame\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Running SQL statements by using sql methods provided by spark\n",
    "AdultNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 23 AND 60\")\n",
    "AdultNamesDF.show()\n",
    "\n",
    "# Alternatively, creating a dataframe for a JSON dataset represented by\n",
    "# an RDD[String] storing one JSON object per string\n",
    "jsonStrings = ['{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}']\n",
    "otherPeopleRDD = sc.parallelize(jsonStrings)\n",
    "otherPeople = spark.read.json(otherPeopleRDD)\n",
    "otherPeople.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|key|  value|\n",
      "+---+-------+\n",
      "|238|val_238|\n",
      "| 86| val_86|\n",
      "|311|val_311|\n",
      "| 27| val_27|\n",
      "|165|val_165|\n",
      "|409|val_409|\n",
      "|255|val_255|\n",
      "|278|val_278|\n",
      "| 98| val_98|\n",
      "|484|val_484|\n",
      "|265|val_265|\n",
      "|193|val_193|\n",
      "|401|val_401|\n",
      "|150|val_150|\n",
      "|273|val_273|\n",
      "|224|val_224|\n",
      "|369|val_369|\n",
      "| 66| val_66|\n",
      "|128|val_128|\n",
      "|213|val_213|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os.path import expanduser, join, abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Pointing warehouse location to the default location for managed databases and tables\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark as existing SparkSession\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")\n",
    "spark.sql(\"LOAD DATA LOCAL INPATH 'kv1.csv' INTO TABLE src\")\n",
    "\n",
    "# Queries in HiveQL\n",
    "spark.sql(\"SELECT * FROM src\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    3000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregation queries \n",
    "spark.sql(\"SELECT COUNT(*) FROM src\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: None\n",
      "Key: 0, Value: None\n",
      "Key: 0, Value: None\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 2, Value: val_2\n",
      "Key: 2, Value: None\n",
      "Key: 2, Value: val_2\n",
      "Key: 2, Value: val_2\n",
      "Key: 2, Value: val_2\n",
      "Key: 2, Value: val_2\n",
      "Key: 4, Value: val_4\n",
      "Key: 4, Value: None\n",
      "Key: 4, Value: val_4\n",
      "Key: 4, Value: val_4\n",
      "Key: 4, Value: val_4\n",
      "Key: 4, Value: val_4\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: None\n",
      "Key: 5, Value: None\n",
      "Key: 5, Value: None\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 8, Value: val_8\n",
      "Key: 8, Value: None\n",
      "Key: 8, Value: val_8\n",
      "Key: 8, Value: val_8\n",
      "Key: 8, Value: val_8\n",
      "Key: 8, Value: val_8\n",
      "Key: 9, Value: val_9\n",
      "Key: 9, Value: None\n",
      "Key: 9, Value: val_9\n",
      "Key: 9, Value: val_9\n",
      "Key: 9, Value: val_9\n",
      "Key: 9, Value: val_9\n"
     ]
    }
   ],
   "source": [
    "# The dataFrames as result of SQL queries all normal functions.\n",
    "sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\")\n",
    "\n",
    "# The items in DataFrames of type Row, accessing each column by ordinal\n",
    "stringsDS = sqlDF.rdd.map(lambda row: \"Key: %d, Value: %s\" % (row.key, row.value))\n",
    "for record in stringsDS.collect():\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-----+\n",
      "|key|value|key|value|\n",
      "+---+-----+---+-----+\n",
      "|  2|val_2|  2|val_2|\n",
      "|  2|val_2|  2|val_2|\n",
      "|  2|val_2|  2|val_2|\n",
      "|  2|val_2|  2|val_2|\n",
      "|  2|val_2|  2| null|\n",
      "|  2|val_2|  2|val_2|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  4|val_4|  4| null|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "+---+-----+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrames to create temporary views within a SparkSession.\n",
    "Record = Row(\"key\", \"value\")\n",
    "recordsDF = spark.createDataFrame([Record(i, \"val_\" + str(i)) for i in range(1, 101)])\n",
    "recordsDF.createOrReplaceTempView(\"records\")\n",
    "\n",
    "# Joining DataFrame data with data stored in Hive.\n",
    "spark.sql(\"SELECT * FROM records r JOIN src s ON r.key = s.key\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Machine learning Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the neccesary modules/packages\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import SparkSession                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the session \n",
    "if __name__ == \"__main__\":\n",
    "   spark = SparkSession\\\n",
    "       .builder\\\n",
    "       .appName(\"PipelineExample\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the training documents from a list of id, text and label tuples\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Configuring an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the pipeline to the training dataframe\n",
    "model = pipeline.fit(training)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the test dataframe\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.159640773879,0.840359226121], prediction=1.000000\n",
      "(5, l m n) --> prob=[0.837832568548,0.162167431452], prediction=0.000000\n",
      "(6, spark hadoop spark) --> prob=[0.0692663313298,0.93073366867], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.982157533344,0.0178424666556], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Prediction on the test set and making predictions\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "     rid, text, prob, prediction = row\n",
    "     print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the neccesary modules/packages\n",
    "from __future__ import print_function\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the sc\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the session\n",
    "if __name__ == \"__main__\":\n",
    "     spark = SparkSession \\\n",
    "          .builder \\\n",
    "          .appName(\"CorrelationExample\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " # create the dataframe\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "            (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "            (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "             (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[ 1.        ,  0.05564149,         nan,  0.40047142],\n",
      "             [ 0.05564149,  1.        ,         nan,  0.91359586],\n",
      "             [        nan,         nan,  1.        ,         nan],\n",
      "             [ 0.40047142,  0.91359586,         nan,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "# Producing the Pearson correlation matrix\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation matrix:\n",
      "DenseMatrix([[ 1.        ,  0.10540926,         nan,  0.4       ],\n",
      "             [ 0.10540926,  1.        ,         nan,  0.9486833 ],\n",
      "             [        nan,         nan,  1.        ,         nan],\n",
      "             [ 0.4       ,  0.9486833 ,         nan,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "# Producing the Spearman correlation matrix\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the neccesary modules/packages\n",
    "from __future__ import print_function\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a new session\n",
    "if __name__ == \"__main__\":\n",
    "     spark = SparkSession\\\n",
    "          .builder\\\n",
    "          .appName(\"LogisticRegressionWithElasticNet\")\\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training data\n",
    "training = spark.read.format(\"libsvm\").load(\"libsvm.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "lrModel = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (692,[244,263,272,300,301,328,350,351,378,379,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.35398352419e-05,-9.10273850559e-05,-0.000194674305469,-0.000203006424735,-3.14761833149e-05,-6.84297760266e-05,1.58836268982e-05,1.40234970914e-05,0.00035432047525,0.000114432728982,0.000100167123837,0.00060141093038,0.000284024817912,-0.000115410847365,0.000385996886313,0.000635019557424,-0.000115064123846,-0.00015271865865,0.000280493380899,0.000607011747119,-0.000200845966325,-0.000142107557929,0.000273901034116,0.00027730456245,-9.83802702727e-05,-0.000380852244352,-0.000253151980086,0.000277477147708,-0.000244361976392,-0.00153947446876,-0.000230733284113])\n",
      "Intercept: 0.22456315961250325\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinominal method for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "mlrModel = mlr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial coefficients: 2 X 692 CSRMatrix\n",
      "(0,244) 0.0\n",
      "(0,263) 0.0001\n",
      "(0,272) 0.0001\n",
      "(0,300) 0.0001\n",
      "(0,350) -0.0\n",
      "(0,351) -0.0\n",
      "(0,378) -0.0\n",
      "(0,379) -0.0\n",
      "(0,405) -0.0\n",
      "(0,406) -0.0006\n",
      "(0,407) -0.0001\n",
      "(0,428) 0.0001\n",
      "(0,433) -0.0\n",
      "(0,434) -0.0007\n",
      "(0,455) 0.0001\n",
      "(0,456) 0.0001\n",
      "..\n",
      "..\n",
      "Multinomial intercepts: [-0.120658794459,0.120658794459]\n"
     ]
    }
   ],
   "source": [
    "# Printing the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Extract the summary from the LogisticRegressionModel trained earlier\n",
    "trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.6833149135741672\n",
      "0.6662875751473734\n",
      "0.6217068546034618\n",
      "0.6127265245887887\n",
      "0.6060347986802873\n",
      "0.6031750687571562\n",
      "0.5969621534836274\n",
      "0.5940743031983118\n",
      "0.5906089243339022\n",
      "0.5894724576491042\n",
      "0.5882187775729587\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the objective per iteration and printing the result\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "     print(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|0.017543859649122806|\n",
      "|0.0| 0.03508771929824561|\n",
      "|0.0| 0.05263157894736842|\n",
      "|0.0| 0.07017543859649122|\n",
      "|0.0| 0.08771929824561403|\n",
      "|0.0| 0.10526315789473684|\n",
      "|0.0| 0.12280701754385964|\n",
      "|0.0| 0.14035087719298245|\n",
      "|0.0| 0.15789473684210525|\n",
      "|0.0| 0.17543859649122806|\n",
      "|0.0| 0.19298245614035087|\n",
      "|0.0| 0.21052631578947367|\n",
      "|0.0| 0.22807017543859648|\n",
      "|0.0| 0.24561403508771928|\n",
      "|0.0|  0.2631578947368421|\n",
      "|0.0|  0.2807017543859649|\n",
      "|0.0|  0.2982456140350877|\n",
      "|0.0|  0.3157894736842105|\n",
      "|0.0|  0.3333333333333333|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "areaUnderROC: 1.0\n"
     ]
    }
   ],
   "source": [
    " # Printing the receiver-operating characteristic as a dataframe and the areaUnderROC\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression_4c878e704f2ac6767c63"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Setting the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "          .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|         threshold|           F-Measure|\n",
      "+------------------+--------------------+\n",
      "|0.7845860015371142|0.034482758620689655|\n",
      "|0.7843193344168922| 0.06779661016949151|\n",
      "|0.7842976092510131|                 0.1|\n",
      "|0.7842531051133191| 0.13114754098360656|\n",
      "|0.7835792429453297| 0.16129032258064516|\n",
      "|0.7835223585829078|  0.1904761904761905|\n",
      "| 0.783284563364102|             0.21875|\n",
      "|0.7832449070254992| 0.24615384615384614|\n",
      "|0.7830630257264691|  0.2727272727272727|\n",
      "|0.7830068256743365| 0.29850746268656714|\n",
      "|0.7822341175907138|  0.3235294117647059|\n",
      "| 0.782111826902122| 0.34782608695652173|\n",
      "| 0.781220790993743|  0.3714285714285714|\n",
      "|0.7802700864854707|  0.3943661971830986|\n",
      "|0.7789683616171501|  0.4166666666666667|\n",
      "|0.7789606764592472|  0.4383561643835616|\n",
      "|0.7788060694625324| 0.45945945945945943|\n",
      "|0.7783754276111222|  0.4799999999999999|\n",
      "|0.7771658291080574|                 0.5|\n",
      "|0.7769914303593917|  0.5194805194805194|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Printing the f measure results\n",
    "fMeasure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(max(F-Measure)=1.0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Printing the max FMeasure\n",
    "maxFMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5585022394278357"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Printing the best Threshold\n",
    "bestThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 Reccomendation systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the neccesary modules/packages\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "if sys.version >= '3':long = int\n",
    "    \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Building a new session\n",
    "if __name__ == \"__main__\":\n",
    "     spark = SparkSession\\\n",
    "          .builder\\\n",
    ".appName(\"ALSExample\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Creating the dataframe\n",
    "rdd = sc.textFile('ratings2.csv')\n",
    "rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df= rdd.map(lambda line: Row(userId=line[0], \n",
    "movieId=line[1],\n",
    "rating=line[2], \n",
    "timestamp=line[3])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+------+\n",
      "|movieId|rating| timestamp|userId|\n",
      "+-------+------+----------+------+\n",
      "|      2|   3.5|1112486027|     1|\n",
      "|     29|   3.5|1112484676|     1|\n",
      "|     32|   3.5|1112484819|     1|\n",
      "|     47|   3.5|1112484727|     1|\n",
      "|     50|   3.5|1112484580|     1|\n",
      "+-------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Show the dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Casting the features/variables to integers\n",
    "from pyspark.sql.types import IntegerType\n",
    "df=df.withColumn(\"userId\", df[\"userId\"].cast(IntegerType()))\n",
    "df=df.withColumn(\"movieId\", df[\"movieId\"].cast(IntegerType()))\n",
    "df=df.withColumn(\"rating\", df[\"rating\"].cast(IntegerType()))\n",
    "df=df.withColumn(\"timestamp\", df[\"timestamp\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Splitting the train and test in 80/20\n",
    "(training, test) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the recommendation model using ALS on the train data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "                      coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.9332826538500137\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model by printing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                                   predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generating top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|  1580|[[7241, 17.19252]...|\n",
      "|  4900|[[4282, 10.177976...|\n",
      "|  5300|[[7058, 6.7370977...|\n",
      "|  6620|[[67665, 8.95527]...|\n",
      "|   471|[[96911, 8.252678...|\n",
      "|  1591|[[43744, 10.89861...|\n",
      "|  4101|[[6813, 7.4541287...|\n",
      "|  1342|[[4282, 13.613807...|\n",
      "|  2122|[[6339, 10.289984...|\n",
      "|  2142|[[5174, 11.00116]...|\n",
      "|   463|[[67665, 9.871851...|\n",
      "|   833|[[72714, 13.74275...|\n",
      "|  5803|[[32525, 8.633098...|\n",
      "|  3794|[[8527, 13.211122...|\n",
      "|  6654|[[3847, 11.462468...|\n",
      "|  1645|[[75341, 9.10404]...|\n",
      "|  3175|[[93270, 13.23678...|\n",
      "|  4935|[[72714, 9.80378]...|\n",
      "|   496|[[1529, 7.6244574...|\n",
      "|  2366|[[72714, 12.70508...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing the user recommendations\n",
    "userRecs.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|movieId|     recommendations|\n",
      "+-------+--------------------+\n",
      "|   1580|[[4747, 6.1958365...|\n",
      "|   4900|[[2559, 12.581707...|\n",
      "|   5300|[[6322, 14.011656...|\n",
      "|   6620|[[5361, 7.4421635...|\n",
      "|   7240|[[3220, 9.055042]...|\n",
      "|   7340|[[4747, 11.626975...|\n",
      "|   7880|[[992, 5.989811],...|\n",
      "|  32460|[[2940, 13.113006...|\n",
      "|  54190|[[1247, 8.309697]...|\n",
      "|  57370|[[1773, 5.8368864...|\n",
      "|    471|[[3159, 7.8786597...|\n",
      "|   1591|[[1229, 7.117619]...|\n",
      "|   4101|[[6240, 12.982417...|\n",
      "|  80451|[[3159, 3.1412215...|\n",
      "|   1342|[[999, 9.297892],...|\n",
      "|   2122|[[2559, 9.091963]...|\n",
      "|   2142|[[5361, 8.768823]...|\n",
      "|   7982|[[4143, 8.795311]...|\n",
      "|  33722|[[1773, 9.470557]...|\n",
      "|  44022|[[6427, 8.61894],...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing the movie recommendations\n",
    "movieRecs.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
