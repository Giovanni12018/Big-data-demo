{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python superscript V1\n",
    "\n",
    "#### Table of content\n",
    "\n",
    "    1 Spark\n",
    "    1.1 Load modules/packages\n",
    "    1.2 Spark connection setup\n",
    "    1.3 RDD operations\n",
    "    1.4 DF's and data-manipulation\n",
    "    1.5 Machine learning pipelines\n",
    "    1.6 Correlation\n",
    "    1.7 Logistic regression\n",
    "    1.8 Recommendation system\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load modules/packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the neccesary modules/packages\n",
    "from pyspark import SparkContext\n",
    "from __future__ import print_function\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### 1.2 Spark connection setup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkContext from pyspark\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sc\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 RDD operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a list with names and values \n",
    "data = sc.parallelize([('Jordan',100),('Jason',150),('Jack',200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data example \n",
    "data_example = sc.parallelize([('Jordan',100),('Jason',150),('Jack',200)]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jordan', 100), ('Jason', 150), ('Jack', 200)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the data structure of the RDD\n",
    "data_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Jack', 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returning 'Jack'\n",
    "data_example[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a new list\n",
    "data_second = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a distributed dataset\n",
    "distData = sc.parallelize(data_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Reducing the elements in the list by adding them up\n",
    "distData.reduce(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 DF's and data-manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting and initiating the Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"Python Spark SQL basic example\") \\\n",
    " .config(\"spark.some.config.option\", \"some-value\") \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in and attach the CSV\n",
    "df = spark.read.csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the datatype\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run summary statistics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the naâ€™s in the dataframe\n",
    "df.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling null values\n",
    "df.fillna(0).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Machine learning Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the neccesary modules/packages\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import SparkSession                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the session \n",
    "if __name__ == \"__main__\":\n",
    "   spark = SparkSession\\\n",
    "       .builder\\\n",
    "       .appName(\"PipelineExample\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the training documents from a list of id, text and label tuples\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Configuring an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the pipeline to the training dataframe\n",
    "model = pipeline.fit(training)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the test dataframe\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.159640773879,0.840359226121], prediction=1.000000\n",
      "(5, l m n) --> prob=[0.837832568548,0.162167431452], prediction=0.000000\n",
      "(6, spark hadoop spark) --> prob=[0.0692663313298,0.93073366867], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.982157533344,0.0178424666556], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Prediction on the test set and making predictions\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "     rid, text, prob, prediction = row\n",
    "     print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the neccesary modules/packages\n",
    "from __future__ import print_function\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the sc\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the session\n",
    "if __name__ == \"__main__\":\n",
    "     spark = SparkSession \\\n",
    "          .builder \\\n",
    "          .appName(\"CorrelationExample\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " # create the dataframe\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "            (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "            (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "             (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[ 1.        ,  0.05564149,         nan,  0.40047142],\n",
      "             [ 0.05564149,  1.        ,         nan,  0.91359586],\n",
      "             [        nan,         nan,  1.        ,         nan],\n",
      "             [ 0.40047142,  0.91359586,         nan,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "# Producing the Pearson correlation matrix\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation matrix:\n",
      "DenseMatrix([[ 1.        ,  0.10540926,         nan,  0.4       ],\n",
      "             [ 0.10540926,  1.        ,         nan,  0.9486833 ],\n",
      "             [        nan,         nan,  1.        ,         nan],\n",
      "             [ 0.4       ,  0.9486833 ,         nan,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "# Producing the Spearman correlation matrix\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the neccesary modules/packages\n",
    "from __future__ import print_function\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a new session\n",
    "if __name__ == \"__main__\":\n",
    "     spark = SparkSession\\\n",
    "          .builder\\\n",
    "          .appName(\"LogisticRegressionWithElasticNet\")\\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training data\n",
    "training = spark.read.format(\"libsvm\").load(\"libsvm.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "lrModel = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (692,[244,263,272,300,301,328,350,351,378,379,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.35398352419e-05,-9.10273850559e-05,-0.000194674305469,-0.000203006424735,-3.14761833149e-05,-6.84297760266e-05,1.58836268982e-05,1.40234970914e-05,0.00035432047525,0.000114432728982,0.000100167123837,0.00060141093038,0.000284024817912,-0.000115410847365,0.000385996886313,0.000635019557424,-0.000115064123846,-0.00015271865865,0.000280493380899,0.000607011747119,-0.000200845966325,-0.000142107557929,0.000273901034116,0.00027730456245,-9.83802702727e-05,-0.000380852244352,-0.000253151980086,0.000277477147708,-0.000244361976392,-0.00153947446876,-0.000230733284113])\n",
      "Intercept: 0.22456315961250325\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinominal method for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "mlrModel = mlr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial coefficients: 2 X 692 CSRMatrix\n",
      "(0,244) 0.0\n",
      "(0,263) 0.0001\n",
      "(0,272) 0.0001\n",
      "(0,300) 0.0001\n",
      "(0,350) -0.0\n",
      "(0,351) -0.0\n",
      "(0,378) -0.0\n",
      "(0,379) -0.0\n",
      "(0,405) -0.0\n",
      "(0,406) -0.0006\n",
      "(0,407) -0.0001\n",
      "(0,428) 0.0001\n",
      "(0,433) -0.0\n",
      "(0,434) -0.0007\n",
      "(0,455) 0.0001\n",
      "(0,456) 0.0001\n",
      "..\n",
      "..\n",
      "Multinomial intercepts: [-0.120658794459,0.120658794459]\n"
     ]
    }
   ],
   "source": [
    "# Printing the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Extract the summary from the LogisticRegressionModel trained earlier\n",
    "trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.6833149135741672\n",
      "0.6662875751473734\n",
      "0.6217068546034618\n",
      "0.6127265245887887\n",
      "0.6060347986802873\n",
      "0.6031750687571562\n",
      "0.5969621534836274\n",
      "0.5940743031983118\n",
      "0.5906089243339022\n",
      "0.5894724576491042\n",
      "0.5882187775729587\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the objective per iteration and printing the result\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "     print(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|0.017543859649122806|\n",
      "|0.0| 0.03508771929824561|\n",
      "|0.0| 0.05263157894736842|\n",
      "|0.0| 0.07017543859649122|\n",
      "|0.0| 0.08771929824561403|\n",
      "|0.0| 0.10526315789473684|\n",
      "|0.0| 0.12280701754385964|\n",
      "|0.0| 0.14035087719298245|\n",
      "|0.0| 0.15789473684210525|\n",
      "|0.0| 0.17543859649122806|\n",
      "|0.0| 0.19298245614035087|\n",
      "|0.0| 0.21052631578947367|\n",
      "|0.0| 0.22807017543859648|\n",
      "|0.0| 0.24561403508771928|\n",
      "|0.0|  0.2631578947368421|\n",
      "|0.0|  0.2807017543859649|\n",
      "|0.0|  0.2982456140350877|\n",
      "|0.0|  0.3157894736842105|\n",
      "|0.0|  0.3333333333333333|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "areaUnderROC: 1.0\n"
     ]
    }
   ],
   "source": [
    " # Printing the receiver-operating characteristic as a dataframe and the areaUnderROC\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression_4c878e704f2ac6767c63"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Setting the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "          .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|         threshold|           F-Measure|\n",
      "+------------------+--------------------+\n",
      "|0.7845860015371142|0.034482758620689655|\n",
      "|0.7843193344168922| 0.06779661016949151|\n",
      "|0.7842976092510131|                 0.1|\n",
      "|0.7842531051133191| 0.13114754098360656|\n",
      "|0.7835792429453297| 0.16129032258064516|\n",
      "|0.7835223585829078|  0.1904761904761905|\n",
      "| 0.783284563364102|             0.21875|\n",
      "|0.7832449070254992| 0.24615384615384614|\n",
      "|0.7830630257264691|  0.2727272727272727|\n",
      "|0.7830068256743365| 0.29850746268656714|\n",
      "|0.7822341175907138|  0.3235294117647059|\n",
      "| 0.782111826902122| 0.34782608695652173|\n",
      "| 0.781220790993743|  0.3714285714285714|\n",
      "|0.7802700864854707|  0.3943661971830986|\n",
      "|0.7789683616171501|  0.4166666666666667|\n",
      "|0.7789606764592472|  0.4383561643835616|\n",
      "|0.7788060694625324| 0.45945945945945943|\n",
      "|0.7783754276111222|  0.4799999999999999|\n",
      "|0.7771658291080574|                 0.5|\n",
      "|0.7769914303593917|  0.5194805194805194|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Printing the f measure results\n",
    "fMeasure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(max(F-Measure)=1.0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Printing the max FMeasure\n",
    "maxFMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5585022394278357"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Printing the best Threshold\n",
    "bestThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 Reccomendation systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the neccesary modules/packages\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "if sys.version >= '3':long = int\n",
    "    \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Building a new session\n",
    "if __name__ == \"__main__\":\n",
    "     spark = SparkSession\\\n",
    "          .builder\\\n",
    ".appName(\"ALSExample\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Creating the dataframe\n",
    "rdd = sc.textFile('ratings2.csv')\n",
    "rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df= rdd.map(lambda line: Row(userId=line[0], \n",
    "movieId=line[1],\n",
    "rating=line[2], \n",
    "timestamp=line[3])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+------+\n",
      "|movieId|rating| timestamp|userId|\n",
      "+-------+------+----------+------+\n",
      "|      2|   3.5|1112486027|     1|\n",
      "|     29|   3.5|1112484676|     1|\n",
      "|     32|   3.5|1112484819|     1|\n",
      "|     47|   3.5|1112484727|     1|\n",
      "|     50|   3.5|1112484580|     1|\n",
      "+-------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Show the dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Casting the features/variables to integers\n",
    "from pyspark.sql.types import IntegerType\n",
    "df=df.withColumn(\"userId\", df[\"userId\"].cast(IntegerType()))\n",
    "df=df.withColumn(\"movieId\", df[\"movieId\"].cast(IntegerType()))\n",
    "df=df.withColumn(\"rating\", df[\"rating\"].cast(IntegerType()))\n",
    "df=df.withColumn(\"timestamp\", df[\"timestamp\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Splitting the train and test in 80/20\n",
    "(training, test) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the recommendation model using ALS on the train data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "                      coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.9332826538500137\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model by printing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                                   predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generating top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|  1580|[[7241, 17.19252]...|\n",
      "|  4900|[[4282, 10.177976...|\n",
      "|  5300|[[7058, 6.7370977...|\n",
      "|  6620|[[67665, 8.95527]...|\n",
      "|   471|[[96911, 8.252678...|\n",
      "|  1591|[[43744, 10.89861...|\n",
      "|  4101|[[6813, 7.4541287...|\n",
      "|  1342|[[4282, 13.613807...|\n",
      "|  2122|[[6339, 10.289984...|\n",
      "|  2142|[[5174, 11.00116]...|\n",
      "|   463|[[67665, 9.871851...|\n",
      "|   833|[[72714, 13.74275...|\n",
      "|  5803|[[32525, 8.633098...|\n",
      "|  3794|[[8527, 13.211122...|\n",
      "|  6654|[[3847, 11.462468...|\n",
      "|  1645|[[75341, 9.10404]...|\n",
      "|  3175|[[93270, 13.23678...|\n",
      "|  4935|[[72714, 9.80378]...|\n",
      "|   496|[[1529, 7.6244574...|\n",
      "|  2366|[[72714, 12.70508...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing the user recommendations\n",
    "userRecs.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|movieId|     recommendations|\n",
      "+-------+--------------------+\n",
      "|   1580|[[4747, 6.1958365...|\n",
      "|   4900|[[2559, 12.581707...|\n",
      "|   5300|[[6322, 14.011656...|\n",
      "|   6620|[[5361, 7.4421635...|\n",
      "|   7240|[[3220, 9.055042]...|\n",
      "|   7340|[[4747, 11.626975...|\n",
      "|   7880|[[992, 5.989811],...|\n",
      "|  32460|[[2940, 13.113006...|\n",
      "|  54190|[[1247, 8.309697]...|\n",
      "|  57370|[[1773, 5.8368864...|\n",
      "|    471|[[3159, 7.8786597...|\n",
      "|   1591|[[1229, 7.117619]...|\n",
      "|   4101|[[6240, 12.982417...|\n",
      "|  80451|[[3159, 3.1412215...|\n",
      "|   1342|[[999, 9.297892],...|\n",
      "|   2122|[[2559, 9.091963]...|\n",
      "|   2142|[[5361, 8.768823]...|\n",
      "|   7982|[[4143, 8.795311]...|\n",
      "|  33722|[[1773, 9.470557]...|\n",
      "|  44022|[[6427, 8.61894],...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing the movie recommendations\n",
    "movieRecs.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
