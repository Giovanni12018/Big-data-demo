#Table of content

#1 Prediction 1
#1.1 Fast GBM
#1.2 Load libraries
#1.3 Reading in data 
#1.4 Data pre-processing
#1.5 Split train/test
#1.6 Fast GBM 1
#1.7 Fast GBM 2 
#1.8 Fast GBM 3
#1.9 Prediction

#2 Prediction 2 
#2.1 Auto modelling
#2.2 Imputation
#2.3 Hyper-parameter tuning
#2.4 Optimized predictive model GBM bernoulli
#2.5 Prediction

#3 Prediction 3
#3.1 GBM with all variables 
#3.2 Fast GBM 4 
#3.3 Prediction

#4 Prediction 4
#4.1 Ensembling
#4.2 Stacking ensemble

######################################################################################################################
######################################################################################################################
##1) Prediction 1  

#1.1) Fast GBM 

#First we will apply a fast GBM for quick first results and valuable insight in the data.

######################################################################################################################
######################################################################################################################
##1.2) Load libraries

install.packages("")

library(gbm)
library(cvAUC)
library(randomForest)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(MASS)
library(caret)
library(RANN)
library(data.table)
library(xgboost)
library(caretEnsemble)
library(pROC)
library(ggplot2)
library(reshape)
library(mlbench)
library(MLmetrics)
######################################################################################################################
######################################################################################################################
##1.3) Reading in data 

data <- read.csv(file="C:/Santander customer Satisfaction/data.csv")
######################################################################################################################
######################################################################################################################
##1.4) Data pre-processing

# Randomizing the dataset
data<- data[sample(1:nrow(data)),]

#Subset data
data <- data[1:50000,]

#Set seed
set.seed(100)

#Checking data type response variable
is.numeric(data$TARGET)

#Creating clean dataset
myvars <- c("TARGET","var3","var15","var38")
data1 <- data[myvars]

#Count the na's
NAcol <- which(colSums(is.na(data1)) > 0)
cat('There are', length(NAcol), 'columns with missing values')

#Histogram response/dependent variable
ggplot(data1, aes(TARGET)) +
  geom_bar(fill = "#0073C2FF")

#####################################################################################################################
#####################################################################################################################
##1.5) Split train/test

# Step 1: Get row numbers for the training data
partition <- createDataPartition(data1$TARGET, p=0.7, list=FALSE)

# Step 2: Create the training  dataset
train <- data1[partition,]

# Step 3: Create the test dataset
test <- data1[-partition,]
######################################################################################################################
######################################################################################################################
##1.6) Fast GBM 1

set.seed(100)

#1 GBM 1 Bernoulli
gbm1 <- gbm(TARGET~.,                 
           data=train,                
           distribution='bernoulli',  
           n.trees=300,               
           shrinkage=0.1,             
           interaction.depth=1,       
           train.fraction = 1,        
           cv.fold = 5,
           n.minobsinnode = 10,       
           keep.data = TRUE,          
           verbose=F,                 
           n.cores = NULL)  #Use all cores as default                

# plot the performance
best.iter<- gbm.perf(gbm1,method="cv")   
print(best.iter)

#3 Marginal effect plots 
for(i in 1:length(gbm1$var.names)){
  plot(gbm1, i.var = i
       , ntrees= gbm.perf(gbm1, plot.it = FALSE)
       , type = "response"
  )
}

summary(gbm1,n.trees=best.iter) 
######################################################################################################################
######################################################################################################################
##1.7) Fast GBM 2 

#Exclude the ID column
myvars <- names(data) %in% c('ID') 
data <- data[!myvars]

# Step 1: Get row numbers for the training data
partition <- createDataPartition(data$TARGET, p=0.7, list=FALSE)
# Step 2: Create the training  dataset
train <- data[partition,]
# Step 3: Create the test dataset
test <- data[-partition,]

set.seed(100)

#1 GBM 1 Bernoulli
gbm1 <- gbm(TARGET~.,               
            data=train,                
            distribution='bernoulli',  
            n.trees=300,               
            shrinkage=0.1,             
            interaction.depth=1,       
            train.fraction = 1,        
            cv.fold = 5,
            n.minobsinnode = 10,       
            keep.data = TRUE,          
            verbose=F,                 
            n.cores = NULL)  #Use all cores as default                

# check 5-fold cross-validation performance
best.iter <- gbm.perf(gbm1,method="cv")
print(best.iter)

#All the marginal effect plots 
for(i in 1:length(gbm1$var.names)){
  plot(gbm1, i.var = i
       , ntrees= gbm.perf(gbm1, plot.it = FALSE)
       , type = "response"
  )
}

summary(gbm1)

######################################################################################################################
######################################################################################################################
##1.8) Fast GBM 3 

#Creating clean dataset
myvars <- names(data) %in% c("TARGET","var15","var38","saldo_var30")
data1 <- data[myvars]

# Step 1: Get row numbers for the training data
partition <- createDataPartition(data1$TARGET, p=0.7, list=FALSE)
# Step 2: Create the training  dataset
train <- data1[partition,]
# Step 3: Create the test dataset
test <- data1[-partition,]

set.seed(100)

#1 GBM 1 Bernoulli
gbm1 <- gbm(TARGET~.,                 
            data=train,                
            distribution='bernoulli',  
            n.trees=200,               
            shrinkage=0.1,             
            interaction.depth=1,       
            train.fraction = 1,        
            cv.fold = 5,
            n.minobsinnode = 10,       
            keep.data = TRUE,          
            verbose=F,                 
            n.cores = NULL)  #Use all cores as default             

# plot the performance
best.iter<- gbm.perf(gbm1,method="cv")   # returns 5-fold cv estimate of best number of trees
print(best.iter)

#3 Marginal effect plots 
for(i in 1:length(gbm1$var.names)){
  plot(gbm1, i.var = i
       , ntrees= gbm.perf(gbm1, plot.it = FALSE)
       , type = "response"
  )
}

summary(gbm1,n.trees=best.iter) # based on the estimated best number of trees

######################################################################################################################
##1.19 Prediction 

# f.predict on canonical scale (logit,log,etc.)
f.predict <- predict.gbm(gbm1, test, n.trees = best.iter, type='link')
head(f.predict)
str(f.predict)
#OR
f.predict <- predict.gbm(gbm1, test, n.trees = best.iter)
head(f.predict)
str(f.predict)

# transform to probability scale 
p.pred <- 1/(1+exp(-f.predict))
head(p.pred)
str(p.pred)
#OR
# predictions on probability scale
prediction <- predict(gbm1, test, n.trees = best.iter, type='response')
head(prediction)
str(prediction)

test$TARGET <- as.factor(test$TARGET)
is.factor(test$TARGET)

library(magrittr)
density(prediction) %>% plot

# Generate predictions on test dataset
preds <- predict(gbm1, newdata = test, n.trees = best.iter)
labels <- test[,"TARGET"]

# Compute AUC on the test set
cvAUC::AUC(predictions = preds, labels = labels)

test$TARGET <- as.numeric(test$TARGET)

######################################################################################################################
######################################################################################################################
##2) Prediction 2 
                            
#2.1) Auto-modelling 
                            
#First we will apply a fast GBM for quick first results and valuable insight in the data.                            
######################################################################################################################
######################################################################################################################
##2.2 Imputation

#Checking for Na's in dataframe
cat('There are', length(NAcol), 'columns with missing values')

apply(data1, 2, function(x) any(is.na(x)))

####################################################################################################################
####################################################################################################################
#2.3) Hyper parameter tuning

#Creating clean dataset
myvars <- names(data) %in% c("TARGET","var15","saldo_var30")
data1 <- data[myvars]

# Step 1: Get row numbers for the training data
partition <- createDataPartition(data1$TARGET, p=0.7, list=FALSE)
# Step 2: Create the training  dataset
train <- data1[partition,]
# Step 3: Create the test dataset
test <- data1[-partition,]

#Converting the outcome/response to factor
train$TARGET <- as.factor(train$TARGET)
is.factor(train$TARGET)

## 3.4 Parameter Tuning
fitControl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 1)

set.seed(100)

model_gbm<-train(TARGET~.,method='gbm',data=train,distribution="bernoulli",verbose=F,trControl=fitControl,tuneLength=10)

print(model_gbm)

plot(model_gbm)

## 3.4 Parameter Tuning
fitControl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 1)

#Creating grid
grid <- expand.grid(n.trees=c(50,100,150),
                    shrinkage=c(0.01,0.05,0.1),
                    n.minobsinnode = c(10),
                    interaction.depth=c(1,2,3))

set.seed(100)

# training the model
model_gbm<-train(TARGET~.,method='gbm',trControl=fitControl,distribution="bernoulli",verbose=F,data=train,tuneGrid=grid)

print(model_gbm)

plot(model_gbm)

##################################################################################################################
##################################################################################################################
#2.4) Optimized predictive model GBM Bernoulli


## 3.4 Parameter Tuning
fitControl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 1)

#Creating grid
grid <- expand.grid(n.trees=c(100),
                    shrinkage=c(0.01),
                    n.minobsinnode = c(10),
                    interaction.depth=c(1))

set.seed(100)

# training the model
model_gbm<-train(TARGET~.,method='gbm',distribution="bernoulli",trControl=fitControl,verbose=F,data=train,tuneGrid=grid)

print(model_gbm)

summary(model_gbm)
#OR
plot(varImp(object=model_gbm),main="GBM - Variable Importance")

getTrainPerf(model_gbm)

confusionMatrix(model_gbm)

##################################################################################################################
##################################################################################################################
##2.5) Prediction - 

gbmpred <- predict(model_gbm, test)
head(gbmpred)
str(gbmpred)
#OR
gbmpred <- predict(model_gbm, test, type = "raw")
head(gbmpred)
str(gbmpred)

gbmprobs <- predict(model_gbm, test, type = "prob")
head(gbmprobs)
str(gbmprobs)

test$TARGET <- as.factor(test$TARGET)
is.factor(test$TARGET)

gbmprobs$obs = test$TARGET
mnLogLoss(gbmprobs, lev = levels(gbmprobs$obs))

postResample(gbmpred, test$TARGET)

caret::confusionMatrix(gbmpred, test$TARGET)

rocCurve <- roc(response = test$TARGET,
                predictor = gbmprobs[, "1"],
                levels = rev(levels(test$TARGET)))
rocCurve
plot(rocCurve)
#plot(rocCurve,
     #print.thres = c(.5,.2),
     #print.thres.pch = 16,
     #print.thres.cex = 1.2)


mPred = predict(model_gbm, test, na.action = na.pass)

mResults = predict(model_gbm, test, na.action = na.pass, type = "prob")
mResults$obs = test$TARGET

mnLogLoss(mResults, lev = levels(mResults$obs))

gbmprobs$pred = predict(model_gbm, test, na.action = na.pass)
multiClassSummary(gbmprobs, lev = levels(gbmprobs$obs))

evalResults <- data.frame(Class = test$TARGET)
evalResults$GBM <- predict(model_gbm, test, na.action = na.pass, type = "prob")

head(evalResults)

######################################################################################################################
######################################################################################################################
##3) Prediction 3 

#3.1) GBM with all variables 

######################################################################################################################
######################################################################################################################
##3.2 Fast GBM 4

#Creating clean dataset
myvars <- c("TARGET","var38","num_var22_ult1","saldo_var30","num_var22_hace2","num_var22_ult3",
            "saldo_var37","saldo_medio_var5_hace2","num_var45_hace2","num_var45_ult1","saldo_var5",
            "num_var45_ult3","num_var45_hace3")
data1 <- data[myvars]


# Step 1: Get row numbers for the training data
partition <- createDataPartition(data1$TARGET, p=0.7, list=FALSE)
# Step 2: Create the training  dataset
train <- data1[partition,]
# Step 3: Create the test dataset
test <- data1[-partition,]

train$TARGET <- as.numeric(train$TARGET)
is.numeric(train$TARGET)

set.seed(100)

#1 GBM 1 Bernoulli
gbm1 <- gbm(TARGET~.,                  # formula
            data=train,                # dataset
            distribution='bernoulli',  # loss function: Bernoulli
            n.trees=300,              # number of trees
            shrinkage=0.1,             # learning rate
            interaction.depth=1,       # number splits per tree
            train.fraction = 1,        # fraction of data for training
            cv.fold = 5,
            n.minobsinnode = 10,       # minimum number of obs for split
            keep.data = TRUE,          # store copy of input data in model
            verbose=F,                 # Don't print progress information 
            n.cores = NULL)            # Use all cores as default               

# check performance using 5-fold cross-validation
best.iter <- gbm.perf(gbm1,method="cv")
print(best.iter)

#Marginal effect plots 
for(i in 1:length(gbm1$var.names)){
  plot(gbm1, i.var = i
       , ntrees= gbm.perf(gbm1, plot.it = FALSE)
       , type = "response"
  )
}

summary(gbm1)

######################################################################################################################
##3.3 Prediction

predict(gbm1, newdata = head(test), type = "response")

library(magrittr)
density(prediction) %>% plot

# Generate predictions on test dataset
preds <- predict(gbm1, newdata = test, n.trees = best.iter)
labels <- test[,"TARGET"]

# Compute AUC on the test set
cvAUC::AUC(predictions = preds, labels = labels)

######################################################################################################################
######################################################################################################################
##4) Prediction 4 

#4.1) Ensembling 

#For the third prediction we'll use an ensemble model.

##################################################################################################################
##################################################################################################################
##4.2) Stacking ensemble

#Creating clean dataset
myvars <- names(data) %in% c("TARGET","var15","var38","saldo_var30")
data1 <- data[myvars]

# Step 1: Get row numbers for the training data
partition <- createDataPartition(data1$TARGET, p=0.7, list=FALSE)
# Step 2: Create the training  dataset
train2 <- data1[partition,]
# Step 3: Create the test dataset
test2 <- data1[-partition,]

train2$TARGET[train$TARGET=="0"] <- "No"
train2$TARGET[train$TARGET=="1"] <- "Yes"

set.seed(100)

# create submodels
control <- trainControl(method="repeatedcv", number=2, repeats=1, savePredictions=TRUE, classProbs=TRUE)
algorithmList <- c('glm', 'xgbTree','rf')
set.seed(100)
models <- caretList(TARGET~., data=train2, trControl=control, methodList=algorithmList)
results <- resamples(models)
summary(results)
dotplot(results)

modelCor(results)
splom(results)

set.seed(100)

# stack using gbm
stackControl <- trainControl(method="repeatedcv", number=5, repeats=1, savePredictions=TRUE, classProbs=TRUE)
set.seed(100)
stack.glm <- caretStack(models, method="gbm", metric="Accuracy", trControl=stackControl)
print(stack.glm)

gbmpred <- predict(stack.glm, test2)
head(gbmpred)
str(gbmpred)
#OR
gbmpred <- predict(stack.glm, test2, type = "raw")
head(gbmpred)
str(gbmpred)

gbmprobs <- predict(stack.glm, test2, type = "prob")
head(gbmprobs)
str(gbmprobs)

##################################################################################################################
##################################################################################################################
##5) Optimization

#Optimization 1: In-depth EDA 

#Optimization 2: Etc.

#Optimization 3: 

#Optimization 4: 

#Optimization 5:

#Optimization 6: 
